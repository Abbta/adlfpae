{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbta/adlfpae/blob/main/Exercise_9_autoencoders_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK4CPGb7O84"
      },
      "source": [
        "## Autoencoders Lab\n",
        "#### visualizing HapMap phase 3 populations\n",
        "\n",
        "**This is a solution template. Every chunk of code requiring your input will begin with the # TASK comment and all places where you should fill-in with your code are marked by ellipsis (...).**\n",
        "\n",
        "### Stage 0 &mdash; getting the data\n",
        "\n",
        "First, we will download the data from the linked Dropbox account. The code is hidden as it is not super important here. Double-click below if you are curious to see it!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR5Tn0OA6S-L"
      },
      "source": [
        "#@title Load raw data. Double click to see the code. { display-mode: \"form\" }\n",
        "\n",
        "!wget https://www.dropbox.com/s/g7862q1l4ls9z3x/autosomal_5k_matrix.csv\n",
        "!wget https://www.dropbox.com/s/3lv0062dw20qdqg/autosomal_5k_phenos.csv\n",
        "!wget https://www.dropbox.com/s/6nzrusxkm536a5j/autosomal_5k_kinship.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVM1F-zSDhs8"
      },
      "source": [
        "Now, we will load the data and make sure they look as expected. Note, the genotypes per individual (row) are encoded as the count of minor alleles and thus can take values `gt = {0, 1, 2}`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_v8Lv8ZVu0v"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"autosomal_5k_matrix.csv\", header=0, index_col=0)\n",
        "pheno = pd.read_csv(\"autosomal_5k_phenos.csv\", header=0, index_col=0)\n",
        "\n",
        "print(data)\n",
        "print(pheno)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9bJbHksPJ3q"
      },
      "source": [
        "Now, we will create a dictionary and re-name our populations so that the names are a bit more informative:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iRDgDj-H7mZK",
        "outputId": "bd521e6d-b244-4122-be58-c88b590695e5"
      },
      "source": [
        "pop_dict = {'ASW':'African ancestry in SW USA',\n",
        "            'CEU':'Utah residents with N and W European ancestry',\n",
        "            'CHB':'Han Chinese in Beijing China',\n",
        "            'CHD':'Chinese in Metropolitan Denver Colorado',\n",
        "            'GIH':'Gujarati Indians in Houston Texas',\n",
        "            'JPT':'Japanese in Tokyo Japan',\n",
        "            'LWK':'Luhya in Webuye Kenya',\n",
        "            'MEX':'Mexican ancestry in Los Angeles California',\n",
        "            'MKK':'Maasai in Kinyawa Kenya',\n",
        "            'TSI':'Toscans in Italy',\n",
        "            'YRI':'Yoruba in Ibadan Nigeria'}\n",
        "pheno2 = pheno.replace({\"population\": pop_dict})\n",
        "pheno2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sex</th>\n",
              "      <th>FID</th>\n",
              "      <th>dad</th>\n",
              "      <th>mom</th>\n",
              "      <th>pheno</th>\n",
              "      <th>population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NA19919</th>\n",
              "      <td>NA19919</td>\n",
              "      <td>1</td>\n",
              "      <td>2427</td>\n",
              "      <td>NA19908</td>\n",
              "      <td>NA19909</td>\n",
              "      <td>0</td>\n",
              "      <td>African ancestry in SW USA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19916</th>\n",
              "      <td>NA19916</td>\n",
              "      <td>1</td>\n",
              "      <td>2431</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>African ancestry in SW USA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19835</th>\n",
              "      <td>NA19835</td>\n",
              "      <td>0</td>\n",
              "      <td>2424</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>African ancestry in SW USA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA20282</th>\n",
              "      <td>NA20282</td>\n",
              "      <td>0</td>\n",
              "      <td>2469</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>African ancestry in SW USA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19703</th>\n",
              "      <td>NA19703</td>\n",
              "      <td>1</td>\n",
              "      <td>2368</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>African ancestry in SW USA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19119</th>\n",
              "      <td>NA19119</td>\n",
              "      <td>1</td>\n",
              "      <td>Y060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yoruba in Ibadan Nigeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA18860</th>\n",
              "      <td>NA18860</td>\n",
              "      <td>1</td>\n",
              "      <td>Y012</td>\n",
              "      <td>NA18859</td>\n",
              "      <td>NA18858</td>\n",
              "      <td>0</td>\n",
              "      <td>Yoruba in Ibadan Nigeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19207</th>\n",
              "      <td>NA19207</td>\n",
              "      <td>1</td>\n",
              "      <td>Y051</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yoruba in Ibadan Nigeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19103</th>\n",
              "      <td>NA19103</td>\n",
              "      <td>1</td>\n",
              "      <td>Y042</td>\n",
              "      <td>NA19101</td>\n",
              "      <td>NA19102</td>\n",
              "      <td>0</td>\n",
              "      <td>Yoruba in Ibadan Nigeria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NA19099</th>\n",
              "      <td>NA19099</td>\n",
              "      <td>0</td>\n",
              "      <td>Y105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yoruba in Ibadan Nigeria</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1184 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  sex   FID  ...      mom pheno                  population\n",
              "NA19919  NA19919    1  2427  ...  NA19909     0  African ancestry in SW USA\n",
              "NA19916  NA19916    1  2431  ...        0     0  African ancestry in SW USA\n",
              "NA19835  NA19835    0  2424  ...        0     0  African ancestry in SW USA\n",
              "NA20282  NA20282    0  2469  ...        0     0  African ancestry in SW USA\n",
              "NA19703  NA19703    1  2368  ...        0     0  African ancestry in SW USA\n",
              "...          ...  ...   ...  ...      ...   ...                         ...\n",
              "NA19119  NA19119    1  Y060  ...        0     0    Yoruba in Ibadan Nigeria\n",
              "NA18860  NA18860    1  Y012  ...  NA18858     0    Yoruba in Ibadan Nigeria\n",
              "NA19207  NA19207    1  Y051  ...        0     0    Yoruba in Ibadan Nigeria\n",
              "NA19103  NA19103    1  Y042  ...  NA19102     0    Yoruba in Ibadan Nigeria\n",
              "NA19099  NA19099    0  Y105  ...        0     0    Yoruba in Ibadan Nigeria\n",
              "\n",
              "[1184 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FL19dojZ1b3"
      },
      "source": [
        "# TASK Scaling\n",
        "# We need to scale our counts data so that it is bound between 0 and 1.\n",
        "\n",
        "geno_data = ...\n",
        "geno_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkdzkQeUaX9a"
      },
      "source": [
        "# TASK Randomly split into the training and the validation set, so that 80 per-cent of individuals ends up in the training set.\n",
        "\n",
        "train = geno_data.sample(frac = ..., random_state = 42)\n",
        "test = geno_data.drop(train.index)\n",
        "train.reset_index()\n",
        "test.reset_index()\n",
        "\n",
        "# TASK Print some info about the resulting split\n",
        "print(\"Total number of individuals:\", ...)\n",
        "print(\"\\t - training set:\", ...)\n",
        "print(\"\\t - test set:\", ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU4jM1w8dG0D",
        "outputId": "a09953a8-5889-432d-8270-85d3758c790b"
      },
      "source": [
        "# TASK Specify the autoencoder model\n",
        "\n",
        "train_tensor = train.to_numpy()\n",
        "print(type(train_tensor))\n",
        "\n",
        "# TASK Hyperparameters\n",
        "# use ReLu activations, ADAM optimizer and\n",
        "# mean squared error as the loss function\n",
        "hp_loss_fn = ...\n",
        "hp_act_fn = ...\n",
        "hp_optimizer = ...\n",
        "hp_metrics = ['mse','mae','mape']\n",
        "\n",
        "input_data = keras.Input(shape = (train_tensor.shape[1],))\n",
        "\n",
        "# TASK Define architecture of the encoder:\n",
        "# the second layer should be a batch normalization\n",
        "\n",
        "def Encoder(input):\n",
        "  # Encoder\n",
        "  layer1 = layers.Dense(units = 1500, activation = hp_act_fn, name='layer1')(input)\n",
        "  ...\n",
        "  layer3 = layers.Dropout(rate = 0.05, name='layer3')(layer2)\n",
        "  layer4 = layers.Dense(units = 250, activation = hp_act_fn, name='layer4')(layer3)\n",
        "  layer5 = layers.Dropout(rate = 0.025, name='layer5')(layer4)\n",
        "  layer6 = layers.Dense(units = 25, activation = hp_act_fn, name='layer6')(layer5)\n",
        "  bottleneck = layers.Dense(units = 2, name='layer_bottleneck')(layer6)\n",
        "  return(bottleneck)\n",
        "\n",
        "# TASK Look at the encoder, complete the decoder function\n",
        "\n",
        "def Decoder(bottleneck):\n",
        "  # Decoder\n",
        "  layer7 = layers.Dense(units = 25, activation = hp_act_fn)(bottleneck)\n",
        "  ...\n",
        "  layer12 = layers.Dense(units = train_tensor.shape[1], activation = 'sigmoid')(layer11)\n",
        "  return(layer12)\n",
        "\n",
        "def Autoencoder(input):\n",
        "  enc = Encoder(input)\n",
        "  autoenc = Decoder(enc)\n",
        "  return(autoenc)\n",
        "\n",
        "autoencoder_model = keras.Model(inputs = input_data, outputs = Autoencoder(input_data))\n",
        "autoencoder_model.compile(\n",
        "  loss = hp_loss_fn,\n",
        "  optimizer = hp_optimizer,\n",
        "  metrics = hp_metrics\n",
        ")\n",
        "\n",
        "# TASK Visualise the created architecture and summarise its parameters\n",
        "..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5000)]            0         \n",
            "                                                                 \n",
            " layer1 (Dense)              (None, 1500)              7501500   \n",
            "                                                                 \n",
            " layer2 (BatchNormalization)  (None, 1500)             6000      \n",
            "                                                                 \n",
            " layer3 (Dropout)            (None, 1500)              0         \n",
            "                                                                 \n",
            " layer4 (Dense)              (None, 250)               375250    \n",
            "                                                                 \n",
            " layer5 (Dropout)            (None, 250)               0         \n",
            "                                                                 \n",
            " layer6 (Dense)              (None, 25)                6275      \n",
            "                                                                 \n",
            " layer_bottleneck (Dense)    (None, 2)                 52        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 25)                75        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 250)               6500      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 250)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1500)              376500    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5000)              7505000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,777,152\n",
            "Trainable params: 15,774,152\n",
            "Non-trainable params: 3,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfb6ey7bndRc"
      },
      "source": [
        "# TASK Set hyperparameters for model fitting\n",
        "# Begin by training for 30 epochs, with mini-batch of 256 and validation set\n",
        "# having 20 per-cent of examples\n",
        "\n",
        "...\n",
        "\n",
        "autoencoder = autoencoder_model.fit(x = train_tensor,\n",
        "                      y = train_tensor,\n",
        "                      epochs = hp_epochs,\n",
        "                      batch_size = hp_batch_size,\n",
        "                      shuffle = True,\n",
        "                      validation_split = hp_val_split,\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwdEFIC1cFPb"
      },
      "source": [
        "Now, let us look at the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9bm8NQGMzqM"
      },
      "source": [
        "loss = autoencoder.history['loss']\n",
        "val_loss = autoencoder.history['val_loss']\n",
        "epochs = range(hp_epochs)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Qn_e2QcLQL"
      },
      "source": [
        "Now, that the model is trained, we can save the weights and use them to build an encoder. Note that weights are saved for the entire autoencoder, so we need to use `skip_mismatch = True` along with `by_name = True` to initialize weights in our encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cwxgo-HPRSn"
      },
      "source": [
        "autoencoder_model.save_weights('autoencoder_weights.weights.h5',\n",
        "                               overwrite = True)\n",
        "\n",
        "encoder_model = keras.Model(inputs = input_data, outputs = Encoder(input_data))\n",
        "encoder_model.load_weights('autoencoder_weights.weights.h5',\n",
        "                           skip_mismatch = True)\n",
        "encoder_model.compile(\n",
        "  loss = hp_loss_fn,\n",
        "  optimizer = hp_optimizer,\n",
        "  metrics = hp_metrics,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDovZnxcpiS"
      },
      "source": [
        "Let us embed our genotyping data using the encoder we have just constructed.\n",
        "We can also visualise the embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfJ1GMyZST6N"
      },
      "source": [
        "embeded_points = encoder_model.predict(geno_data.to_numpy())\n",
        "print(embeded_points)\n",
        "\n",
        "x = embeded_points[:,0]\n",
        "y = embeded_points[:,1]\n",
        "pop = pheno2['population']\n",
        "data = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.scatterplot(x='x', y='y', data=data, hue='pop', style='pop', s=100)\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cu093dtc3V8"
      },
      "source": [
        "Now, we will compare the result with:\n",
        "* MDS on the kinship matrix\n",
        "* PCA perfored directly on raw genotypes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wncBFt7pl77e"
      },
      "source": [
        "kinship = pd.read_csv(\"autosomal_5k_kinship.csv\", header=0, index_col=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1JVviGiTFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada0200b-94ad-4087-e7d2-3d7d494c9c84"
      },
      "source": [
        "from sklearn.manifold import MDS\n",
        "embedding = MDS(n_components=2)\n",
        "mds_embedding = embedding.fit_transform(kinship)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_mds.py:419: UserWarning: The MDS API has changed. ``fit`` now constructs an dissimilarity matrix from data. To use a custom dissimilarity matrix, set ``dissimilarity='precomputed'``.\n",
            "  warnings.warn(\"The MDS API has changed. ``fit`` now constructs an\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuTDjHaiuZ6"
      },
      "source": [
        "# TASK Plot MDS embedding in a way similar to plotting autoencoder embeddings\n",
        "x = ...\n",
        "y = ...\n",
        "data = {'x':x, 'y':y, 'pop':pop}\n",
        "plt.figure(figsize = (10,10))\n",
        "...\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_MYJAJ8deKh"
      },
      "source": [
        "Finally, we will perform PCA on raw genotypes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZznrf_q1gE2"
      },
      "source": [
        "# TASK perform PCA with 2 components on raw genotypes (use geno_data as input but remember it has been scaled)\n",
        "# Visualise the result.\n",
        "from sklearn.decomposition import PCA\n",
        "embedding = PCA(...)\n",
        "pca_embedding = ...\n",
        "...\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, markerscale=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}